{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# Introduction\n\nFor the 9th Workshop in Biomedical Engineering (http://wbme.fc.ul.pt/) a specific workshop was prepared to showcase how some of IBM Watson's API could be used; given the target audience and the overall themes of the WBME I had the following \n\n* Use one or more Watson APIs\n* Build a use-case which makes sense within the context of the WBME\n* Make the workshop interactive but also useful for those just watching\n* Have as little software requirements as possible\n* Make it useful even after the workshop, which includes being able to distribute it in some way\n\nAfter some days experimenting with different options I settled on:\n\n* IBM Watson Visual Recognition API\n* A dermoscopic database of images that show skin lesions, both benign and malignate: the PH(2) database from the ADDI project (https://www.fc.up.pt/addi/ph2%20database.html).\n* Use IBM Data Science Experience to create, share and enable all to work on their one copies with no software requirements by making use of the Jupyter notebook support with enhanced user interface.\n\nThe initial developed was done using the `Rmd` format and it only required slight changes; one of the changes required the use of a specific zip file for the PH(2) dataset since the original one was in a RAR file and this compression mechanism isn't supported directly in R, which means that I was decompressing it using a system command, not something feasible when using a Spark execution environment. That was the _only_ change to the original PH(2) archive (which means that there is not actual change in the contents).\n\nI would like to thank the 8th WBME organisers for the opportunity and in particular to all of those who attended the workshop and that actively participated in it; a particular thank you to Sara Lobo (Biomedical Engineering, FCT/NOVA) in which laptop we ended up working together for the final part of the workshop that ended (successfully!) on the Student's Room of the Faculty of Sciences of the University of Lisbon more in the style of a group debugging session than a formal workshop - much to the advantage of all involved.\n\n\n## Concerning the topic\n\nThe use if visual recognition for detection of malignant skin lesions is something that has been studied intensively and has been the topic of many specialised papers and studies. While I've chose this topic due to being aligned with a bio-medical engineering event the goal of the Watson API workshop is to explain how to use the Visual Recognition API, not to build a useful model for something with such implications as melanoma detection.\n\nI've added some relevant bibliography that clearly show the complexity and the possible approaches which are possible, including image pre-processing, the importance of masking, different strategies in terms of classification and many more, for those who want to explore the matter in more depth.", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Setting up the environment\n\nThroughout this paper we will need to use and install some `R` packages, and given the interactive nature of the notebook it is better to add all of those initial requirements in a single code statement, that way we can run it once and be done with it.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Load the `caret` library, which includes the partitioning function we will use\nlibrary(caret)\n## Package e1071 is used by `caret` for the confusion matrix\ninstall.packages('e1071', dependencies=TRUE)\n# Install needed packages for image conversion\ninstall.packages(\"bmp\")\ninstall.packages(\"jpeg\")\ninstall.packages(\"pixmap\")\n## Load the image-related libraries\nlibrary(bmp)\nlibrary(jpeg)\nlibrary(pixmap)\nlibrary(jpeg)\nlibrary(grid)\n## For the REST API\nlibrary(jsonlite)\nlibrary(httr)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "An IBM Bluemix (https://www.ibm.com/cloud-computing/bluemix/) account is needed; during the workshop this was the first step done but the process is simple: sign up and create a free account and get access to the entire catalogue, including Watson APIs.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Obtaining and preparing the data\n\nThe first step we are going to take is downloading the PH(2) database and preparing the data; these step is fundamental in terms of Data Science and we will spend some time going through it step-by-step to make sure that we have all the needed actions properly automated.\n\nThe focus on having everything done \"in code\" is important since it enables repeatability, which is crucial.\n\n## Cleaning up\n\nThis seems strange: why clean up when we are just starting? Given that we are in an interactive (and iteractive) environment which keeps data between runs it is assumed that we will want to run the code several times, and this first step is to remove all the files which we will create during this document. If there is nothing created yet it's not a problem: the `unlink` function deletes what exists and if it doesn't it does nothing.\n\nThere is a sole exception: we will not delete de compressed archive since it doesn't change and this avoids having to download the same context several times.\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Delete previous runs\nzip_dir <-\"ph2-training-files\"\n## If present remove existing archives from previous runs\nunlink(zip_dir, recursive=T,force=T)\nunlink(\"training-set-positive.zip\", force=T)\nunlink(\"training-set-negative.zip\", force=T)\nunlink(\"parameters.json\", force=T)\nunlink(\"*.csv\", force=T)\nunlink(\"PH2Dataset\", force=T, recursive=T)\n## Uncomment to delete the database zip as well\n##unlink(\"PH2Dataset.zip\", force=T)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Getting the dataset\n\nThe first action is to download the PH(2) database, available as a downloadable compressed archive; we set up some variables - the URL where the file is and the name of the zip file - and check for the zip file locally, bypassing the download if it already existing.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## New dataset\nph2_url  <- \"https://www.dropbox.com/s/9a962jfcrs5x4iq/PH2Dataset.zip?dl=0\"\nph2_archive <- \"PH2Dataset.zip\"\n\n### Check for zipped dataset file: if it doesn't exist then download it\nif (!file.exists(ph2_archive)) {\n    print(\"Downloading PH(2) archive\")\n    download.file(ph2_url, destfile=ph2_archive, method=\"wget\")\n} else {\n    print(\"File already exists, skipping download\")\n}\n", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We should have the PH(2) zip zile (and just that file) in our working directory.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "dir()", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Now that we have the archive we unzip it.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "dir(\".\")\nunzip(ph2_archive)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Initial cleanup\n\nThe structure of the newly created directory", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "dir(\"PH2Dataset\")", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Of particular interest to us is the `PH2_dataset.txt` file that contains a text-based description of the obervations. Let's read the file into a variable and examine some of the content.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "ph2_txt <- readLines(\"PH2Dataset/PH2_dataset.txt\")\nhead(ph2_txt)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "It's a file best read using a monospaced font but one can clearly see that it has a header and then the observations, which columns divided by vertical bars; this is something easy for us to understand but not yet ideal to be used programatically, so we will do some transformations to convert it into something that uses a comma to separate the differente fields - a CSV file which can be directly imported into R, and which we will write to disk (something optional since we could just use the result of the transformations directly).", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Convert the txt to a csv file; due to the format this requires\n## several operations.\nph2_new <- gsub (\"^\\\\|\\\\|\", \"\", fixed=FALSE,ph2_txt[1:201]) # Delete the || in the beginning of the file \nph2_new <- gsub (\"\\\\|\\\\|$\", \"\", fixed=FALSE,ph2_new)        # Delete the || at the end of the file\nph2_new <- gsub (\"||\", \",\", fixed=T,ph2_new)                # Replace all the remaining || with a comma\nph2_new <- gsub (\"|\", \",\", fixed=T,ph2_new)                 # Replace all the | with a comma\n\n## Save the result to a file\nwriteLines(ph2_new, \"ph2_dataset.csv\")\n## ... and read that file into a variable\nph2_table <- read.csv(\"ph2_dataset.csv\", header=T)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We now have a CSV file in our working directory,", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "dir()", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "... and a `ph2_table` variable which is an R data frame resulting from the import of the CSV file.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "str(ph2_table)\nhead(ph2_table)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Creating the training and testing sets\n\nSome columns are interpreted as numeric although they are factors; additionally we remove the \"atypical nevus\" images since for our purposes a smaller dataset is desirable since we want to keep the training time of the model small enough and also avoid reaching the daily allowance of images; this has the effect of removing a group of images which are ambiguous and will certainly result in a model which is simples in the sense that it works has been trained with less corner cases.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "ph2_table$Clinical.Diagnosis <- as.factor(ph2_table$Clinical.Diagnosis)\nph2_table$Asymmetry <- as.factor(ph2_table$Asymmetry)\nph2 <- ph2_table[!ph2_table$Clinical.Diagnosis == \"1\", ]\nph2$Clinical.Diagnosis <- droplevels(ph2$Clinical.Diagnosis)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## We only have two factors now\nprint(ph2$Clinical.Diagnosis)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We now split the training data into two different sets, training (70%) and testing (30%), using the Clinical Diagnosis as the outcome", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "inTrain <- createDataPartition(y=ph2$Clinical.Diagnosis, p=0.7, list=FALSE)\ntraining_set <- ph2[inTrain, ]\ntesting_set <-  ph2[-inTrain, ]", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## See what we have so far\nhead(training_set)\nhead(testing_set)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Our approach is straightforward and does not consider the need for\noversampling/bootstraping: we will use the same proportion present in\nthe pruned dataset.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "print(sprintf(\"Common Nevus: %s%%\", round(nrow(ph2[ph2$Clinical.Diagnosis == \"0\", ]) / nrow(ph2) *100, 0)))\nprint(sprintf(\"Melanoma: %s%%\",     round(nrow(ph2[ph2$Clinical.Diagnosis == \"2\", ]) / nrow(ph2) *100, 0)))\n## Plot\nbarplot(table(ph2$Clinical.Diagnosis))", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Positive and negative\n\nTo use IBM Watson's Visual Recognition we need both positive and\nnegative sets to test the network; we will use our training set and\ndivide it into a positive set which includes the melanoma diagnosis\nand a negative one which includes the rest.\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Total number of cases\nsprintf(\"Total number of cases in training set: %d\",nrow(training_set))\n## Benign and malignal cases in the training set\n\nsprintf(\"Total number of bening cases in training set: %d\", nrow(training_set[training_set$Clinical.Diagnosis == \"0\",]))\nsprintf(\"Total number of malignant cases in training set: %d\", nrow(training_set[training_set$Clinical.Diagnosis == \"2\",]))\n\n## Calculate the percentages...\nprint(sprintf(\"Common Nevus: %s%%\", round(nrow(training_set[training_set$Clinical.Diagnosis == \"0\", ]) / nrow(training_set) *100, 0)))\nprint(sprintf(\"Melanoma: %s%%\",     round(nrow(training_set[training_set$Clinical.Diagnosis == \"2\", ]) / nrow(training_set) *100, 0)))\n\n## ... and plot them\nbarplot(table(training_set$Clinical.Diagnosis))\n\n## Create variables for both sets to make it clearer\ntraining_positive <- (training_set[training_set$Clinical.Diagnosis == \"2\",])\ntraining_negative <- (training_set[training_set$Clinical.Diagnosis == \"0\",])\n", 
            "metadata": {
                "collapsed": false, 
                "scrolled": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Files and paths\n\nThe Visual Recognition API accepts the images as a zip file so we use\nthe image names to build the adequate archives, but before let's explore the database through the directory structure in a bit more detail.\n\nAs we saw we have at the toplevel some files and a directory:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "dir(\"PH2Dataset\")", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Inside the images directory there are other directories, one for each file. We will use an option to `dir` to obtain the full path of the files and directories to make it clearer.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "dir(\"PH2Dataset/PH2 Dataset images\", full.names=T)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Each of these directories has a similar structure, and we will use a random one to list all the contents.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "name_sample <- training_set[sample.int(nrow(training_set),1), ]\nname_sample$Name", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "To get there we just need to build the file path - this will be very important in the next step - by simply appending the file name to the images directory.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "dir(sprintf(\"PH2Dataset/PH2 Dataset images/%s\", name_sample$Name), full.names=T, recursive=T)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Is that directory empty? We actually know it isn't (you can download the archive and check it yourself), so what happened? The answer lies in the `Name` field: if we check it a bit better we can see that it contains extra characters.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "sprintf(\"PH2Dataset/PH2 Dataset images/%s\", name_sample$Name)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Since we didn't clean the `Name` column properly it contains extra whitespace, which is passed into `dir`. The solution is to trim all the names, removing any whitespace.\n\nNote that this, as many other actions we will perform, would be better done to the initial `ph2_table` data frame that contains the complete number of observations and from which we would then extract the needed datasets, but since we're learning as we go we will correct the derived datasets seperately.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Trim function: remove unneeded whitespace\ntrim <- function (x) gsub(\"^\\\\s+|\\\\s+$\", \"\", x)\ntraining_set$Name <- trim(training_set$Name)\ntesting_set$Name <- trim(testing_set$Name)", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "With this fixed let's see how it works.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Get a new sample\nname_sample <- training_set[sample.int(nrow(training_set),1), ]\n## Check the file path\nsprintf(\"PH2Dataset/PH2 Dataset images/%s\", name_sample$Name)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Looks correct so now we can try to obtain the directory listing of the image directory again, and hopefuly it will work now.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "dir(sprintf(\"PH2Dataset/PH2 Dataset images/%s\", name_sample$Name), full.names=T, recursive=T)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Great, it worked.\n\nThe image we are going to use if the one in the _Dermoscopy Image_ directory, and we will not use any of the two (one always present, the other optional) additional images made available which are _masks_ which identify the lesion itself from the surrounding tissue. This is explained in the `Readme.txt` file, which should actually be the very first thing we should read in order to understand the database so let's take a look now.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "readLines(\"PH2Dataset/Readme.txt\")\n", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We are now able to add a correct file image path to each observation and so we add a new column - `File` - that contains that information.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Trim the entries in the positive and negative sets\ntraining_positive$Name <- trim(training_positive$Name)\ntraining_negative$Name <- trim(training_negative$Name)\n## Add the file image path to each observation\ntraining_positive$File <- sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.bmp\", training_positive$Name, training_positive$Name, training_positive$Name)\ntraining_negative$File <- sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.bmp\", training_negative$Name, training_negative$Name, training_negative$Name)\n## Since we could forget about this latter we will do the same to the testing set right now\ntesting_set$File <- sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.bmp\", testing_set$Name, testing_set$Name, testing_set$Name)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We can now see that we have the file path in an additional column", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "head(training_positive)\nhead(training_negative)\nhead(testing_set)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Image format conversion\n\nEverything looks fine... but there's nothing like making sure things are actually working so we will try to load and display an image from two random samples.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Pick a random sample from the training set\nnegative_sample <- training_negative[sample.int(nrow(training_negative),1), ]\npositive_sample <- training_positive[sample.int(nrow(training_positive),1), ]\n\n## Use read.bmp to read the image and then create a pixmapRGB object that can be \"plotted\"\nnegative_image <- pixmapRGB(read.bmp(negative_sample$File))\npositive_image <- pixmapRGB(read.bmp(positive_sample$File))\n\n## We use plot to display the image, and par to display them in a single row, side by side\n##par(mfrow=c(1,2))\nplot(negative_image, sub = negative_sample$Name)\nplot(positive_image, sub = positive_sample$Name)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Now that we have confirmed that the image paths are correct we need to build the zip archives; Watson Visual recognition receives the images as zip files which contain all the images that will be used to build the model.\n\nFirst we create some auxiliary directories that will be used to make the two zip files.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "zip_dir <-\"ph2-training-files\"\npositive_dir <- file.path(zip_dir,\"positive\")\nnegative_dir <- file.path(zip_dir,\"negative\")\n\ndir.create(zip_dir)\ndir.create(positive_dir)\ndir.create(negative_dir)\ndir(zip_dir)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We just need to copy the images and create the zip right? Actually, there is still one important thing missing: Visual Recognition accepts files in JPEG and PNG format, and the PH(2) dataset has the files as BMP. We need to convert them first, something that is also possible with some additional R libraries.\n\nEach image will be read, converted to JPEG and saved in the same directory as the original, and we will then copy them to specific directories that will be used to build the _positive_ and _negative_ archives. These approach has the advantage of working for the testing set without any change - remember that there is no \"positive\" or \"negative\" split in the testing set.\n\nDo note that, as always, there are many different ways to go about this transformation; in the initial version the `magick` library was used and it makes the process quite straightforward and arguably more readable. The downside is that it depends on external libraries that make it less portable, hence the use of the `jpeg` and `bmp` libraries.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Pick a new sample\nnegative_sample <- training_negative[sample.int(nrow(training_negative),1), ]\nnegative_image <- pixmapRGB(read.bmp(negative_sample$File))\n##Write the JPEG\nwriteJPEG(getChannels(negative_image), target =  sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", negative_sample$Name, negative_sample$Name, negative_sample$Name), quality = 1)\n\n## Read the JPEG\njpeg <- readJPEG(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", negative_sample$Name, negative_sample$Name, negative_sample$Name))\n\n## Display the JPEG\ngrid.raster(jpeg)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Now that we have a way to convert to JPEG we will convert all the images; this means looping through all observations, and for that we make a function that encapsulates the conversion process.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## This function converts a given image (provided as the name, without extension) to JPEG and saves it as a side-effect before returning the JPEG image as the return value.\nbmp_to_jpeg <- function (name) {\n    ## Disable warnings from pixmapRGB\n    oldw <- getOption(\"warn\")\n    options(warn = -1)\n\n    ## Read the BMP file\n    bmp_image <- pixmapRGB(read.bmp(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.bmp\", name, name, name)))\n    ## Write the JPEG\n    writeJPEG(getChannels(bmp_image), target =  sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", name, name, name), quality = 1)\n    #print(dir(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image\", name, name)))\n\n    ## Read the JPEG and use it as the return value\n    #readJPEG(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", name, name, name))\n    options(warn = oldw)\n}", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Now that we can read a BMP file, convert into JPEG and save it we just need to apply these steps to all cases in the datasets", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Apply the transformation to all rows\nfor (i in 1:nrow(training_positive))\n{\n   bmp_to_jpeg(training_positive[i,\"Name\"])   \n}\nprint(\"Done\")", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Now we should have a JPEG image along the original BMP file", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Pick another sample and check if there\npositive_sample <- training_positive[sample.int(nrow(training_positive),1), ]\ndir(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/\", positive_sample$Name, positive_sample$Name))\n\n## Read the JPEG\njpeg <- readJPEG(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", positive_sample$Name, positive_sample$Name, positive_sample$Name))\n## Display the JPEG\ngrid.raster(jpeg)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Now that we have all the images in the positive set as JPEG we need to do the same to the rest of the images - remember, this would be something that could and should be done to the original `ph2_table` since it would only have to be done once.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Do the same for the negative example\nfor (i in 1:nrow(training_negative))\n{\n   bmp_to_jpeg(training_negative[i,\"Name\"])\n}\nprint(\"Done\")", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We also check if it worked for this set:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "negative_sample <- training_negative[sample.int(nrow(training_negative),1), ]\ndir(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/\", negative_sample$Name, negative_sample$Name))\n\n## Read the JPEG\njpeg <- readJPEG(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", negative_sample$Name, negative_sample$Name, negative_sample$Name))\n## Display the JPEG\ngrid.raster(jpeg)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We shouldn't forget about the testing set - we will want to submit images for testing and the same restrictions on file format apply.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Do the same for the testing example\nfor (i in 1:nrow(testing_set))\n{\n   bmp_to_jpeg(testing_set[i,\"Name\"])   \n}\nprint(\"Done\")", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "... and check if it worked.", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "testing_sample <- testing_set[sample.int(nrow(testing_set),1), ]\ndir(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/\", testing_sample$Name, testing_sample$Name))\n\n## Read the JPEG\njpeg <- readJPEG(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", testing_sample$Name, testing_sample$Name, testing_sample$Name))\n## Display the JPEG\ngrid.raster(jpeg)", 
            "metadata": {
                "collapsed": false, 
                "scrolled": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "The `File` field of each observation points to the BMP file; since we want to operate on the JPEG files that we created we simply replace that field (another option woild be to add a new column, thus keeping a reference to both files). ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Get fresh samples again\npositive_sample <- training_positive[sample.int(nrow(training_positive),1), ]\nnegative_sample <- training_negative[sample.int(nrow(training_negative),1), ]\ntesting_sample <- testing_set[sample.int(nrow(testing_set),1), ]\n\nprint(\"Before changing to JPEG\")\nhead(positive_sample)\nhead(negative_sample)\nhead(testing_sample)\n\ntraining_positive$File <- sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", training_positive$Name, training_positive$Name, training_positive$Name)\ntraining_negative$File <- sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", training_negative$Name, training_negative$Name, training_negative$Name)\ntesting_set$File <- sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", testing_set$Name, testing_set$Name, testing_set$Name)\n\npositive_sample <- training_positive[sample.int(nrow(training_positive),1), ]\nnegative_sample <- training_negative[sample.int(nrow(training_negative),1), ]\ntesting_sample <- testing_set[sample.int(nrow(testing_set),1), ]\n\nprint(\"After changing to JPEG\")\nhead(positive_sample)\nhead(negative_sample)\nhead(testing_sample)\n\n## Read the JPEG\n#jpeg <- readJPEG(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", negative_sample$Name, negative_sample$Name, negative_sample$Name))\n\n## Display the JPEG; we use grid.newpage() since otherwise grid.raster() would only show the last image.\ngrid.newpage()\ngrid.raster(readJPEG(positive_sample$File))\ngrid.newpage()\ngrid.raster(readJPEG(negative_sample$File))\ngrid.newpage()\ngrid.raster(readJPEG(testing_set$File))\n", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Creating the archives\n\nLet's review some of the steps we've done up until this point:\n\n- Downloaded the PH(2) archive and extracted it\n- Imported the description of all observations\n- Created the testing and training datasets\n- Added the file path to the observations\n- Converted all images to JPEG\n\nWe're very close to actually starting to use Watson! This preliminary steps are (and I'm stressing this again on purpose) fundamental for any Data Science project: parsing text files and converting images is hardly as attractive as using a Deep Learning framework such as Watson Visual Recognition, but it's a necessary step that when done correctly has a huge positive impact on the ability to repeat and share the experiment.\n\nWatson Visual Recognition trains a model by analysing a zip file with the _positive_ examples, and optionally _negative_ examples. We will use this to train a classifier for melanoma, supplying examples of images which are malignant (the positive archive) and benign (the negative archive).\n\nRemember that we have already created the two directories previously.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "dir(zip_dir, full.names=T)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We can now copy the JPEGs to their respective directory, beginning with the ones form the positive set.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Copy the images in the positive set\nfor (i in 1:nrow(training_positive))\n{\n    print(sprintf(\"%s -> %s\",training_positive[i,\"File\"],positive_dir))\n    file.copy(training_positive[i,\"File\"],positive_dir)\n\n}\nprint(\"Directory listing:\")\ndir(positive_dir)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "... and now the negative set.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Copy the images in the negative set\nfor (i in 1:nrow(training_negative))\n{\n    print(sprintf(\"%s -> %s\",training_negative[i,\"File\"],negative_dir))\n    file.copy(training_negative[i,\"File\"],negative_dir)\n\n}\nprint(\"Directory listing:\")\ndir(negative_dir)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "With the images in the appropriate directories it's just a matter of zipping them to create the two archives.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Create the archives\n\n## Create the positive archive\nWD <- getwd()\nsetwd(positive_dir)\nzip(\"../../training-set-positive.zip\", dir(\".\"), flags=\"-j9X\", zip=\"/usr/bin/zip\")\nsetwd(WD)\n\n## Create the positive archive\nWD <- getwd()\nsetwd(negative_dir)\nzip(\"../../training-set-negative.zip\", dir(\".\"), flags=\"-j9X\", zip=\"/usr/bin/zip\")\nsetwd(WD)\n", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We now have two zip files which contain the positive and negative sets that will be used to train Watson, composed of JPEG images.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "dir(\".\")\nfile.info(\"training-set-positive.zip\")\nfile.info(\"training-set-negative.zip\")", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Using Watson Visual Recognition\n\nIt's (finally!) time to train our model!\n\n## Credentials\n\nWe will need the Visual Recognition API key to submit our datasets for training; this information is available in the Bluemix console after adding the Visual Recognition service, something which can be done by logging in Bluemix (https://console.ng.bluemix.net/login), searching for _Visual Recognition_ in the catalogue and adding the service.\n\nThis will add the Visual Recognition service to your dashboard (you can accept the default values that are suggested during the creation of the service), and in the details of the service there is a 'Service Credentials' pane which will display the list of credentials and a 'View Credentials' option with something similar to the following JSON string:\n\n```json\n{\n  \"url\": \"https://gateway-a.watsonplatform.net/visual-recognition/api\",\n  \"note\": \"It may take up to 5 minutes for this key to become active\",\n  \"api_key\": \"fcccc0bd44bd59db50121xxxx559e6474892yyy-\"\n}\n```\nYou will need to copy the `url` and copy it to the `api_end` variable bellow (by replacing the characters within quotes, if needed since the `url` is likely to be the same) and the `api_key` to the variable with the same name.\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## API Endpoint: change to the correct value obtained from the \"Credentials\" pane in the Visual Recognition service\napi_endp  <-  \"https://gateway-a.watsonplatform.net/visual-recognition/api\"\n### API Key: change to the correct value obtained from the \"Credentials\" pane in the Visual Recognition service\napi_key  <- \"\"", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Creating the model\n\nThe creation of a model is straightforward, but as those who were involved in the workshop  know we need to be extra careful to make the code resilient to multiple runs; the original approach was to imediately make a request for Watson Visual Recognition to create a model, and then save the classifier ID that is generally returned by that call.\n\nThe problem? If one runs the code a second time (on purpose or by mistake) this will fail since the default free plan of Visual Recognition allows for one classifiers to exist at a time, which means that asking for the creation of the model a second time will _not_ return a classifier ID but an error message. This would trickle down the code and essentially render the code cell inoperative.\n\nThe present solution is slightly more complex but mostly because it adds more code, since the approach is quite simple:\n\n1. First, ask for a list of existing classifiers\n2. Check the answer: if a classifier ID exists then by default keep it (but offer the option to delete it by checking a specific variable that )\n3. If there isn't any classifier then create a new one\n\nThis should (hopefuly!) work for all situations and make the code resiliente to multiple runs.\n\nTo help us in interacting with the Visual Recognition REST API (cf. https://watson-api-explorer.mybluemix.net/apis/visual-recognition-v3 for the full API reference) we define some functions to retrieve and delete classifiers.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Retrieve existing classifiers (if any)\nretrieve_classifiers <-  function (api_key, api_endp)\n{\n    req <- GET(sprintf(\"%s/v3/classifiers/?api_key=%s&version=2016-05-20\", api_endp, api_key))\n\n}\n\n## Delete an existing classifier\ndelete_classifier <-  function (api_key, api_endp, classifier_id)\n{\n    req <- DELETE(sprintf(\"%s/v3/classifiers/%s?api_key=%s&version=2016-05-20\", api_endp, classifier_id,api_key))\n\n}\n", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Before proceeding further we must make sure that we have added the corrent information above in the `api_endp` and `api_key`; the following code should return a JSON string\nwith or without a classifier and associated fields (name, status, etc), but it shouldn't return an error message. The following is an example of the most common error which results from an invalid API key (likely because it wasn't properly copied and pasted):\n\n```json\n{\n    \"status\": \"ERROR\",\n    \"statusInfo\": \"invalid-api-key\"\n}\n````\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Get a list of classifiers to test the main API settings\nclassifiers_response  <- retrieve_classifiers(api_key, api_endp)\ncontent(classifiers_response,\"text\")", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "With that hopefuly out of the way we will now check for an existing classifier; we will be using the `jsonlite` library to convert the JSON strings into R objects.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Get the text from the response and parse the JSON into a list\nclassifiers <- fromJSON(content(classifiers_response,\"text\"))\n\n## Check the result\nclassifiers\nstr(classifiers)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "This allows us to more easily work with what the Visual Recognition API answers in terms of the R code we are developing.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Check \nif (any(names(classifiers) == \"classifiers\")) {\n        print(sprintf(\"Existing classifier %s exists, keeping it\", classifiers$classifiers$classifier_id))\n        classifier_id <- classifiers$classifiers$classifier_id\n} else {\n    print(\"No classifiers exist\")\n         classifier_id <- NULL\n}\nclassifier_id", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "The following variable determines the behaviour to addopted when a model already existing and should be changed if a different behaviour is desired; the default value, `keep`, will keep the existing model, change it to `new` to delete and retrain.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## This variable is used to determine what to do when an existing model exists. It can have one of two values:\n##   keep: will keep the existing model and use it\n##   new: will delete the existing model and train a new one\n## \n## It is only checked when an existing model exists.\n\nbehaviour <- \"keep\"", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "The following code uses\n\n1. Information about existing classifiers\n2. The desired behaviour as expressed by the `behaviour` variable\n\nto determine what to do. The code can look daunting but it only does some checking and decides on what to do based on the above two points.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Decide on the training action\nif(is.null(classifier_id)) {\n    ## Classifier is new, train new model\n    print(\"No existing classifier detected, training new model.\")\n    req  <- POST(sprintf(\"%s/v3/classifiers?api_key=%s&version=2016-05-20\", api_endp, api_key), body = list(melanoma_positive_examples=upload_file(\"training-set-positive.zip\"), negative_examples=upload_file(\"training-set-negative.zip\"), name = \"melanoma\"), encode = \"multipart\")\n    reply  <- fromJSON(content(req, \"text\"))\n    classifier_id  <-  reply$classifier_id\n} else {\n    ## Classifier isn't NULL, so we either keep it or retrain it, depending on the variable \"behaviour\"\n    if(behaviour == \"keep\") {\n        ## Keep it\n        print(sprintf(\"Classifier exists and defined behaviour set to keep, reusing classifier %s\", classifier_id))\n    } else if(behaviour == \"new\") {\n        print(\"Classifier exists and defined behaviour set to new, deleting classifier and training new model\")\n        delete_classifier(api_key, api_endp, classifier_id)\n        req  <- POST(sprintf(\"%s/v3/classifiers?api_key=%s&version=2016-05-20\", api_endp, api_key), body = list(melanoma_positive_examples=upload_file(\"training-set-positive.zip\"), negative_examples=upload_file(\"training-set-negative.zip\"), name = \"melanoma\"), encode = \"multipart\")\n        reply  <- fromJSON(content(req, \"text\"))\n        classifier_id  <-  reply$classifier_id\n    } else {\n        print(\"Unknown value for variable \\\"behaviour\\\", please choose \\\"keep\\\" or \\\"new\\\"\")\n    }\n    \n}\n\n##classifier_id <- \n##reply  <- retrieve_classifiers(api_key, api_endp)\n##classifier_id  <-  reply$classifier_id\n##retrieve_classifiers(api_key,api_endp)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We now have a classifier id, either because we created the model for the first time or because we are reusing a previous model.", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "print(classifier_id)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We can check the status of the classifier with a simple `GET` call", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## The following indicates the status of the classifier: it will return \"training\" or \"ready\"\nreq <- GET(sprintf(\"%s/v3/classifiers/%s?api_key=%s&version=2016-05-20\", api_endp, classifier_id, api_key))\njsonlite::prettify(content(req,\"text\"))", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "If it's listed as *ready* we can proceed (this model should take less than 5 minutes to train), so run the following code until it is ready.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## The \"status\" can be used to check if the model is ready, such as in this simple example\nreq <- GET(sprintf(\"%s/v3/classifiers/%s?api_key=%s&version=2016-05-20\", api_endp, classifier_id, api_key))\nif (fromJSON(content(req, \"text\"))$status == \"ready\")\n    {\n        print(\"Classifier ready\")\n    } else {\n        print(\"Classifier in training, please wait\")\n}\n", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We have trained a classifier and now it's time to test it.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Testing the classifier\n\nNow that we have trained our classifier we will use images from our test set to see how Watson classifies them; we define a `classify_image` function to encapsulate the HTTP POST request and make the code simpler to follow.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## This function uses the Visual Recognition API to submit a file for classification\nclassify_image  <- function (api_key, api_endp, image, parameters)\n    {\n        ## parameters should be optional, if absent use the default ones by not uploading anything\n        if(missing(parameters)) {\n            body_list <- list(images_file=upload_file(image))\n        } else {\n            body_list <- list(parameters=upload_file(parameters), images_file=upload_file(image))\n        }\n        POST(sprintf(\"%s/v3/classify?api_key=%s&version=2016-05-20\", api_endp, api_key), body = body_list, encode = \"multipart\")\n    }", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We can start with a single random sample just to see how it works.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Pick a random sample from the training set\nsample <- testing_set[sample.int(nrow(testing_set),1), ]", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Let's remember the structure of our data frame through this single random observation:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "str(sample)\nhead(testing_set)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Using our `classify_image` function we ask Watson to classify our image:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "classification  <- classify_image(api_key, api_endp, sample$File)\n\n## Display the classification result\nresult <- content(classification, \"text\")\nresult\n\n## Read the JPEG\njpeg <- readJPEG(sprintf(\"PH2Dataset/PH2 Dataset images/%s/%s_Dermoscopic_Image/%s.jpg\", sample$Name, sample$Name, sample$Name))\n## Display the JPEG\ngrid.raster(jpeg)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "That's a lot of matches, why? Because we have asked Watson to classify the image based on the default set of classifiers, i.e. existing models which are built-in Watson Visual Recognition and which we can use without having to train any specific classifier; what we want though is to ask Watson to only use the classifier we trained and nothing else: for this we will submit a _parameters_ file which should contain the `classifier_id` of our model.\n\nLet's create the file:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Create the JSON string, save it and display it\nparameters <- \"parameters.json\"\nwrite(toJSON(list(classifier_ids=classifier_id)), parameters)\nprint(readLines(parameters), quote=FALSE)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We can now ask Watson to give us the classification based solely on our classifier:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Ask for classification \nclassification  <- classify_image(api_key, api_endp, parameters, sample$File)\n\n## The request is in binary so we first obtain the text part of the content\nresult <- fromJSON(content(classification, \"text\"))", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "We got our first answer from Watson's Visual Recognition using our model; let's see how what it is:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "print(result)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "The result will vary: if it didn't match then there will be no indication of \"melanoma\" in the output, meaning that for Watson the image is of a benign lesion. We can parse the result to produce a more understandable output.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "if (class(result$images$classifiers[[1]][1]) == \"data.frame\")\n    {\n        ## Only a positive match returns a data frame\n        sample$Watson = 2\n        sample$Score = result$images$classifiers[[1]]$classes[[1]]$score\n        \n    } else {\n        \n        ## A list is returned since there was no match\n        sample$Watson = 0\n        sample$Score = \"NA\"\n        \n    }\n\n sprintf(\"Sample image was %s, Watson identified it as %s (score %s)\", sample$Clinical.Diagnosis, sample$Watson, sample$Score)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Depending on the specific sample we can get a match, a false positive or a false negative (type I and type II error), which will influence _specificity_ and _sensibility_ as we will see.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Classify all the test cases\n\nTo see how accurate is our mode we must now classify all images and store the result so we can latter analyse the results. We will want to store the reply Watson provides along with the score so we'll add two new columns to the testing set. To make the process as clear as possible I've added more `print` statements than usual", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Add new columns to store results\ntesting_set$Watson  <- \"\"\ntesting_set$Score  <- \"\"\n\nsprintf(\"Looping to %s\", nrow(testing_set))\nfor (i in 1:nrow(testing_set))\n{\n    case <- testing_set[i, ]\n    classification  <- classify_image(api_key, api_endp, parameters, case$File)\n    result <- fromJSON(content(classification, \"text\"))\n    print(result)\n\n    ## Depending on the reply we have a match or not\n    if (class(result$images$classifiers[[1]][1]) == \"data.frame\")\n        {\n            ## Only a positive match returns a data frame\n            testing_set[i, ]$Watson = 2\n            testing_set[i, ]$Score = result$images$classifiers[[1]]$classes[[1]]$score\n            \n        } else {\n\n            ## A list is returned since there was no match\n            testing_set[i, ]$Watson = 0\n            \n        }\n    \n    print(sprintf(\"Sample image was %s, Watson identified it as %s (score %s)\", testing_set[i, ]$Clinical.Diagnosis, testing_set[i, ]$Watson, testing_set[i, ]$Score))\n\n}", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Analysing the results\n\nWe now have all the data in our data frame and can make some quick analysis right away by comparing the `Clinical.Diagnosis` field with the `Watson` one:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "testing_set[, c(\"Name\",\"Clinical.Diagnosis\",\"Watson\",\"Score\") ]", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "Since we are using `R` we have at our disposable the necessary tools to make a more in-depth analysis of the results, starting from a simple contingency table:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## A simple table\ntable(testing_set$Clinical.Diagnosis, testing_set$Watson)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "The `caret` library contains a useful function: `confusionMatrix`, exactely what we need to get information on the accuracy, specificity and sensibility of our model:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Confusion Matrix\nconfusionMatrix(table(testing_set$Clinical.Diagnosis, testing_set$Watson))", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "A more visual analysis is provided by a four-fold plot:", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "## Four fold plot\nfourfoldplot(table(testing_set$Clinical.Diagnosis, testing_set$Watson), conf.level = 0,margin=1, color = c(\"#CC6666\", \"#99CC99\"))", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "The results will vary at each run since we are randomly building our training and testing set; in general the model display a very high specificity (with little or no false positives) and a sensibility that varies from very high to high (i.e. some false negatives can appear in some runs).", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Conclusion\n\nIn this workshop we went from getting a zip-file with images to having an image analysis model based on Watson's Visual Recogntion, a \"deep learning\" convolutional neural network that is easily available through Bluemix. We have done a lot of work but barely scratched the surface of what can be done, especially when considering that there are many other Watson APIs which can be used.\n\nSome challenges that could very well be the basis for further workshops and experiments:\n\n* What happens if we do not remove the \"atypical nevus\" entries from the training dataset?\n* How does the current model classify those atypical lesions (hint: it's just a matter of reruning the classifier with a different set)\n* Submitting images via a web interface for classificaiton (hint: Node RED is available on Bluemix and allows for a visual construction of flows with direct support for Watson APIs\n* Applying the masks in the original database increases accuracy? (hint: this would need image conversion and saving the results from several runs; Watson Analytics could be used to analyse the final results)\n* Add a natural language interface that talks to a user, asks for images, uploads them and provides the result (hint: multiple Watson APIs can be used here, from those around speech and text to Watson Conversation).\n\nThese are but some initial ideas, many more are possible and hopefully this small experiment with Visual Recognition will be a starting point.", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "# Bibliography\n\n```\nBhuiyan, M. A. H., Azad, I., & Uddin, M. K. (2013). Image processing for skin cancer features extraction. International Journal of Scientific & Engineering Research, 4(2), 1\u20136.\nCarli, P., Mannone, F., de Giorgi, V., Nardini, P., Chiarugi, A., & Giannotti, B. (2003). The problem of false-positive diagnosis in melanoma screening: the impact of dermoscopy. Melanoma Research, 13(2), 179\u2013182.\nColl, L. R., Chinchilla, D., Coll, C., Stengel, F. M., & Cabo, H. (2010). Digital image analysis of pigmented skin lesions. Early diagnosis of melanoma. Dermatolog\u00eda Argentina, 14(SE), 2008; 14 (3): 200-206.\nDicker, D. T., Lerner, J., Van Belle, P., Guerry, 4th, Herlyn, M., Elder, D. E., & El-Deiry, W. S. (2006). Differentiation of normal skin and melanoma using high resolution hyperspectral imaging. Cancer Biology & Therapy, 5(8), 1033\u20131038.\nGutman, D., Codella, N. C., Celebi, E., Helba, B., Marchetti, M., Mishra, N., & Halpern, A. (2016). Skin Lesion Analysis toward Melanoma Detection: A Challenge at the International Symposium on Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration (ISIC). arXiv Preprint arXiv:1605.01397.\nLinden, A. (2006). Measuring diagnostic and predictive accuracy in disease management: an introduction to receiver operating characteristic (ROC) analysis. Journal of Evaluation in Clinical Practice, 12(2), 132\u2013139.\nMendon\u00e7a, T., Ferreira, P. M., Marques, J. S., Marcal, A. R., & Rozeira, J. (2013). PH 2-A dermoscopic image database for research and benchmarking. In Engineering in Medicine and Biology Society (EMBC), 2013 35th Annual International Conference of the IEEE (pp. 5437\u20135440). IEEE.\nMenegola, A., Fornaciali, M., Pires, R., Bittencourt, F. V., Avila, S., & Valle, E. (2017). Knowledge Transfer for Melanoma Screening with Deep Learning. arXiv Preprint arXiv:1703.07479.\nNylund, A. (2016). To be, or not to be Melanoma: Convolutional neural networks in skin lesion classification.\n```\n", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat_minor": 0, 
    "metadata": {
        "kernelspec": {
            "language": "R", 
            "display_name": "R with Spark 2.0", 
            "name": "r-spark20"
        }, 
        "language_info": {
            "pygments_lexer": "r", 
            "file_extension": ".r", 
            "codemirror_mode": "r", 
            "version": "3.3.2", 
            "mimetype": "text/x-r-source", 
            "name": "R"
        }
    }, 
    "nbformat": 4
}